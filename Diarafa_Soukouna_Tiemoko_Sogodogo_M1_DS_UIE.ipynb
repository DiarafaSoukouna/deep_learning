{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie I : Machine Learning (Questions de cours pratiques)\n",
    "### Partie I/A : Prédiction de la fréquence cardiaque maximale\n",
    "**Dans cette première partie, nous nous concentrons sur l’étude de la fréquence cardiaque maximale atteinte, la variable thalach dans le jeu de données**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Questions préliminaires\n",
    "    - (a) La nature des variables thalach, age, sex et chol :<br>\n",
    "        - **thalach** (\"_fréquence cardiaque maximale atteinte_\") est une variable quantitative discrète.<br>\n",
    "        _La fréquence cardiaque se mesure à travers des valeurs numériques, ce qui en fait une variable quantitative discrète. De plus, le nombre de battements cardiaques se compte de manière entière, et non sous forme d'intervalles ou de nombres décimaux. Cela rend la variable 'thalach' une variable quantitative discrète_\n",
    "        \n",
    "        - **age** (\"_âge du patient_\") est une variable quantitative discrète.<br>\n",
    "        _La variable âge dans le contexte de notre problème est considérée comme une variable quantitative discrète car elle consiste en des valeurs entières et non présentées sous forme d'intervalles._\n",
    "\n",
    "        - **sex** (\"_sexe du patient_\") est une variable catégorielle à deux modalités.<br>\n",
    "\n",
    "        - **chol** (\"_quantité de cholestérol_\") est une variable quantitative discrète car elle consiste en des valeurs entières et non présentées sous forme d'intervalles<br>\n",
    "    \n",
    "    - (b) L'existence d'un lien et la force de ce lien entre la variable chol et thalach :\n",
    "    Pour verifier l'existence d'un lien entre la variable chol et la variable thalach on peut calculer le coefficient de correlation entre les deux variables.\n",
    "    Pour ce faire nous pouvons utiliser la correlation de Pearson (Le coefficient de Pearson est un indice reflétant une relation linéaire entre deux variables continues). Le coefficient de corrélation varie entre -1 et +1, 0 reflétant une relation nulle entre les deux variables, une valeur négative (corrélation négative) signifiant que lorsqu'une des variable augmente, l'autre diminue ; tandis qu'une valeur positive (corrélation positive) indique que les deux variables varient ensemble dans le même sens.<br>\n",
    "    Source: http://www.biostat.ulg.ac.be/pages/Site_r/corr_pearson.html<br>\n",
    "\n",
    "        Processus: Le coefficient de Pearson se calcule comme suit: \n",
    "        $$\n",
    "            r = \\frac{{\\sum (X_i - \\bar{X}) \\cdot (Y_i - \\bar{Y})}}{{\\sqrt{\\sum (X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum (Y_i - \\bar{Y})^2}}}\n",
    "        $$\n",
    "\n",
    "        Avec :\n",
    "        -  X et Y   Nos deux variables entre lesquelles nous souhaitons verifier l'existence d'un lien ( X  pour chol et Y pour thalach );\n",
    "        - r : le coefficient de corrélation de Pearson;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A présent, on souhaite mettre en place un modèle qui nous permettra de prédire la\n",
    "fréquence cardiaque maximale (thalach) d’un patient à partir de différentes variables\n",
    "identifiées comme ayant un effet / lien significatif sur thalach. Dans la suite, on considérera que ces variables identifiées sont: age, sex, trestbps, chol, oldpeak et thal. Un Data\n",
    "scientist nous suggère un modèle de régression linéaire pour ce problème de prédiction.\n",
    "\n",
    "    - (a) Oui la régression linéaire est le modèle adapté pour ce problème.<br>\n",
    "        La régression linéaire est un choix idéal pour notre problème, car il existe une relation linéaire entre notre variable étiquette (thalach) et nos variables indépendantes. De plus, la régression linéaire est relativement simple à traiter en tant que modèle, ce qui en facilite l'interprétation et l'utilisation.\n",
    "    \n",
    "    - (b) La variable étiquette de ce problème est la variable thalach (variable à prédire).\n",
    "    \n",
    "    - (c) Les variables caractéristiques de ce problème sont :\n",
    "        - $x^{[1]}$ : age\n",
    "        - $x^{[2]}$ : sex\n",
    "        - $x^{[3]}$ : trestbps\n",
    "        - $x^{[4]}$ : chol\n",
    "        - $x^{[5]}$ : oldpeak\n",
    "        - $x^{[6]}$ : thal\n",
    "    \n",
    "    - (d) L'expression mathématique du modèle de régression pour le patient i (avec i ∈ {0, . . . , n = 302})  ayant l’étiquette $y_{i}$ et les caractéristiques $x_{i}^{[1]}$, . . ., $x_{i}^{[6]}$, sans oublier le terme d’erreur (résidu) $\\epsilon_{i}$. On note $\\beta_{0}$, $\\beta_{1}$, . . ., $\\beta_{6}$ les paramètres du modèle.\n",
    "    i ∈ {0, . . . , n = 302},<br>\n",
    "    Les caractéristiques :  $ x_i^{[1]},...,x_i^{[6]} $,<br>\n",
    "    Les paramètres :  $\\beta_0,...,\\beta_6 $,<br>\n",
    "    Et le résidu :  $ e_i $.<br>\n",
    "    L'expresion du modèle devient :<br><br>\n",
    "    **$ y_i = \\beta_0 x_i^{[1]} + \\beta_1 x_i^{[2]} + \\beta_2 x_i^{[3]} + \\beta_3 x_i^{[4]} + \\beta_4 x_i^{[5]} + \\beta_5 x_i^{[6]} + e_i $**<br><br>\n",
    "    En mettant les $\\beta$ dans un vecteur et les xi dans un autre on pourra obtenir l'expression suivante : \n",
    "                                        **$$ y_i = \\beta^T \\cdot x_i + e_i $$**\n",
    "\n",
    "    - (e) Afin de pouvoir utiliser notre modèle de régression pour des fins de prédiction, nous devons d’abord l’entraîner.\n",
    "        - i. L'entraînement a pour but de permettre à notre modèle d'apprendre afin qu'il puisse s'imprégner des données fournies et pouvoir prendre des décisions à l'avenir sur de nouvelles données. Il consiste à fournir à notre modèle un grand nombre de données étiquetées afin qu'il puisse s'entraîner, généraliser et être en mesure de prendre des décisions à l'avenir.\n",
    "\n",
    "        - ii. Pour ce problème nous considerons comme fonction perte l'erreur quadratique moyenne.\n",
    "        L'erreur quadratique moyenne serait la fonction de perte la plus adaptée, car elle minimise considérablement les erreurs en se basant sur les différences entre les valeurs prédites et les valeurs réelles. Cela permet d'avoir une idée directe sur la performance du modèle de manière simple et rapide.\n",
    "\n",
    "        - iii. Comme algorithme d'optimisation nous suggérons la methode analytique pour l'entrainement de notre modèle.<br>\n",
    "        Fonctionnement : Cet algorithme consiste à trouver les valeurs des paramètres pour lesquelles la fonction perte serait minimisée.<br>\n",
    "            - Tout d'abord nous calculons les derivées partielles de notre fonction perte;\n",
    "            - Ensuite nous cherchons les points critiques en posant les derivées partielles égales à zéro (les solutions des équations seront nos points critiques);\n",
    "            - Afin de determiner la nature de nos points critiques nous calculons les derivées secondes;\n",
    "            - Et pour finir nous concluons avec cette logique :<br>\n",
    "                - Si la derivée seconde > 0 alors le point critique devient un minimum;\n",
    "                - si la derivée seconde < 0 alors le point critique devient un maximum;\n",
    "                - Si la derivée seconde = 0 alors le point critique devient soit un point selle; soit un point d'inflexion;\n",
    "\n",
    "        - iv. Selon nous, cet algorithme peut être limité en cas de complexité du modèle, ce qui rendrait très difficile la recherche des points critiques. De plus, il pourrait prendre beaucoup de temps à résoudre dans les cas où le modèle comporte de nombreux paramètres, les calculs pourraient prendre beaucoup de temps de calcul.<br>\n",
    "        _**Alternative: L'alternative que nous connaissons est la descente de gradient.**_\n",
    "    \n",
    "    - (f) La séparation des données en données d'entraînement et données de test est importante, car cela nous permet de ne pas exposer toutes nos données au modèle en même temps. Ainsi, le modèle peut apprendre des données d'entraînement et être testé sur de nouvelles données, qui sont les données de test. Ce sont les réponses obtenues lors du test qui nous permettent de mesurer la performance de notre modèle.<br><br>\n",
    "    Les données d'entraînement servent à apprendre au modèle afin qu'il puisse généraliser et prendre des décisions. Les données de test servent à évaluer la performance du modèle en le confrontant à des données qu'il n'a jamais vues.\n",
    "\n",
    "    - (g) Pour évaluer la qualité de notre modèle de régression, nous choisissons le coefficient de détermination R² sur nos données de test. En effet, pour mieux évaluer la qualité du modèle, il est important de le tester sur des données non étiquetées et non vues par le modèle afin d'évaluer sa capacité à généraliser.<br><br>\n",
    "    (Ce coefficient est exprimé sur une échelle de 0 à 1, où un R² de 1 indique que le modèle de régression explique entièrement la variance de la variable dépendante, ce qui signifie une adéquation parfaite entre les valeurs prédites et les valeurs réelles.\n",
    "    À l’inverse, un R² de 0 suggère que le modèle ne parvient pas à expliquer la variance de la variable dépendante, indiquant un ajustement nul du modèle aux données.\n",
    "    Source : https://blog.nalo.fr/lexique/r-carre-ou-r2/)<br><br>\n",
    "    Cette logique rend le coefficient facile à interpréter et à utiliser, offrant des évaluations claires et précises, pouvant même être exprimées sous forme de pourcentage.\n",
    "\n",
    "    - (h) Pour predire la fréquence cardiaque maximale d'un nouveau patient i = 317 nous allons multiplier les carateristiques du patient i par les paramètres estimés de notre modèle.<br><br>\n",
    "    Ainsi la fréquence cardiaque maximale du nouveau patient sera :<br>\n",
    "    **$$ y_317 = \\beta_0 x_317^{[1]} + \\beta_1 x_317^{[2]} + \\beta_2 x_317^{[3]} + \\beta_3 x_317^{[4]} + \\beta_4 x_317^{[5]} + \\beta_5 x_317^{[6]} + e_317 $$**<br><br>\n",
    "    Avec : $x_i$ les carctéritiques respectives age, sex, trestbps, chol, oldpeak et thal.<br><br>\n",
    "    Le resultat de ce calcul sera la fréquence cardiaque maximale de notre patient 317."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARTIE I/B : Prédiction de la présence de la maladie cardiaque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Références :\n",
    "    - https://scikit-learn.org/stable/modules/tree.html#:~:text=DecisionTreeClassifier%20is%20a%20class%20capable,class%20classification%20on%20a%20dataset.&text=In%20case%20that%20there%20are,lowest%20index%20amongst%20those%20classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Notre Data scientist nous suggère ici d’utiliser un modèle de classification pour prédire si un nouveau patient est malade du coeur ou non.\n",
    "\n",
    "    - (a) Êtes vous d’accord avec lui ? Pourquoi ?\n",
    "        - Oui, on est d'accord ! Parceque la donnée à prédire est de classe binaire, donc la prédiction se fait sur deux catégories.\n",
    "    \n",
    "    - (b) Ce problème fait partie de quelle classe de problèmes de Machine Learning ? Justifiez.\n",
    "        - C'est un problème d'apprentissage supervisé ! On connaît les données sur lesquelles le modèle sera formé, donc la prédiction sera faite à partir des données fournies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Suggérez un algorithme de classification pour ce problème de prédiction de la variable target. Pourquoi cet algorithme ? Décrivez brièvement le principe de cet algorithme.\n",
    "    - Notre choix d'algorithme de classification portera sur les **Arbres de décision**.\n",
    "\n",
    "    - Il existe plusieurs algorithmes de classification qu'on peut utiliser en fonction du problème. Mais les arbres de décision sont des algorithmes très simple à utiliser pour un problème de classification binaire.\n",
    "    Toute fois, on peut aussi tester d'autres algorithmes avec celui-ci en faisant des comparaisons et le bon algorithme sera donc celui qui donnera les meilleures perfomances pour notre problème.\n",
    "    \n",
    "    - Alors comment l'arbre de décision (dans notre cas) marche ?\n",
    "    L'algorithme de l'arbre de décision analyse les données et trouve la valeur de la variable prédictive (target) ensuite sépare les données en deux ensembles.\n",
    "    Après la première séparation, il examine chacun des deux sous-ensembles de données et trouve une variable prédictive et les caractéristiques qui donnent le plus d'informations liées à celle-ci (target). Le processus se poursuit jusqu'à ce que la profondeur maximale (max_depth) de l'arbre spécifiée par le programme soit atteinte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Soit $y_{subset}$ = (1, 1, 0, 0, 1, 0, 1, 0, 0, 1) et $\\hat{y}_{subset}$ = (1, 0, 1, 0, 1, 1, 1, 1, 0, 1) respectivement, une partie des vraies étiquettes et des étiquettes prédites (par votre algorithme de classification).\n",
    "    - (a) Proposez un pseudo code (décrivez la procédure algorithmique) permettant de calculer rapidement la  précision (“accuracy”) de votre modèle en partant de $y_{subset}$ et $\\hat{y}_{subset}$.\n",
    "        - Soit les variables : **TN, FN, FP et TP** correspondent respectivement aux nombres de vrais négatifs faux négatifs, faux positifs et vrais positifs;\n",
    "        - Et la variable **precision** (la précision du modèle);\n",
    "        - **precision = TP / (TP + FP)**;\n",
    "        - Afficher la précision du modèle (la variable **precision**)\n",
    "\n",
    "    \n",
    "    - (b) La matrice de confusion à partir des valeurs des vecteurs $y_{subset}$ et $\\hat{y}_{subset}$.\n",
    "        -   |                    | **True 0** |  **True 1** |\n",
    "            |:------------------:|:----------:|:-----------:|\n",
    "            |  **Predicted 0**   |  TN = 2    |  FN = 1     |\n",
    "            |  **Predicted 1**   |  FP = 3    |  TP = 4     |\n",
    "    \n",
    "    - (c) Déduisez de la matrice de confusion : l’“accuracy”, la sensibilité (taux d’individus positifs bien prédits) et la spécificité (taux d’individus négatifs bien prédits) et 1 − spécificité (taux d’individus négatifs mal prédits) de votre modèle de classification.\n",
    "        - **Accuracy = (TP + TN) / (TP + TN + FN + FP)** ==> **Accuracy = 0.6**\n",
    "            - Donc la précision du score de classification est de 60%.\n",
    "\n",
    "        - **La sensibilité (recall) = TP / (TP + FN)** ==> **Sensibilité = 0.8**\n",
    "            - Donc **80%** des individus positifs sont bien prédits.\n",
    "\n",
    "        - **La spécificité = TN / (TN + FP)** ==> **Spécificité = 0.4**\n",
    "            - Donc **40%** des individus négatifs sont bien prédits.\n",
    "\n",
    "        - **1 - spécificité = 0.6**\n",
    "            - Donc **60%** des individus négatifs sont mal prédits.\n",
    "    \n",
    "    - (d) Suggérez une méthode de comparaison de plusieurs modèles de classification binaires.\n",
    "        - On peut comparer plusieurs modèles de classification binaires en faisant :\n",
    "            - Une liste de selection de modèles de classification binaires ;\n",
    "            - On construit le modèle de chaque élément de la liste ;\n",
    "            - Pour chaque élément de la liste, on calcule les différentes mesures de performance telles que la précision, le rappel et la mesure F, ...\n",
    "            - Enfin à travers la performance de chaque modèle, on peut deduire le bon modèle à choisir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie II : Deep Learning - Théorie & Pratique\n",
    "### Partie II/A : Deep Learning - Théorie (Optimisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Les dérivées partielles de la fonction de perte E(W, b) par rapport aux poids et biais de la couche cachée et de la couche de sortie d’un MLP ayant une seule couche cachée. (**Voir le fichier joint**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Déduisez de ces résultats, les formules de mise à jour d’un algorithme de descente de\n",
    "gradient stochastique. (**Voir le fichier joint**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. La différence entre la descente de gradient classique et stochastique :<br>\n",
    "    _La descente de gradient est un algorithme d'optimisation permettant de trouver le minimum d'une fonction convexe._\n",
    "    - La descente de gradient classique (GD) se calcule à travers les derivées partielles par rapport à tous les paramètres du modèle pour tous les données (exemples), ce qui peut-être un gros coûts de calcul pour un gros volume de donnees.\n",
    "\n",
    "    - Alors qu'avec la descente de gradient stochastique (SGD), au lieu de faire le calcul de derivée sur tous les paramètres, on choisit au hasard un exemple dans nos données pour faire ce calcul. Donc elle peut être beaucoup plus rapide que la descente de gradient classique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie II/B : Deep Learning - Pratique\n",
    "##### Deep Learning - Pratique : Réseaux de neurones dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Chargement les données CIFAR10 (60 000 images en couleur réparties en 10 classes, avec 6000 images par classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, losses\n",
    "import matplotlib.pyplot as plt # Pour des affichages graphiques\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Vérifions que les données d'entraînement comportent bien 50 000 images et que les données de test comportent 10 000 images.<br>\n",
    "Pour cela, nous allons afficher la taille des tableaux train_images et test_images à l'aide de la fonction shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(f\"Donc nombre de données d'entrainement est : {train_images.shape[0]}\")\n",
    "print(test_images.shape)\n",
    "print(f\"Donc le nombre de données de test est : {test_images.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus dans le resultat du shape, nous avons la liste suivante : (50000, 32, 32, 3), ce qui signifie que nous avons 32 pixels de largeur des images, 32 pixels de hauteur des images et 3 canaux de couleurs ce qui nous reviendrait au total a $$32 \\times 32 \\times 3 = 3,072 pixels$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Normalisons les données pour que les valeurs des pixels soient entre 0 et 1. Pour cela, nous allons diviser les valeurs de nos variables test_images, train_images par 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images = train_images / 255.0, test_images/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Affichage de quelques images pour montrer une idée des 10 catégories d’images composant\n",
    "l’ensemble de données. Pour cela nous allons utiliser la fonction imshow() de matplotlib.pyplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_some_images(nrows, ncols, class_names = []):\n",
    "  \"\"\"\n",
    "    Cette méthode affiche quelques images de notre dataset sous forme de tableau\n",
    "\n",
    "    nrows : Le nombre de ligne\n",
    "    ncols : Le nombre de colonnes\n",
    "    class_names : La liste des catégories\n",
    "  \"\"\"\n",
    "  fig, axs = plt.subplots(nrows, ncols, figsize=(8, 8), constrained_layout=True) \n",
    "  fig.suptitle('Quelques images')\n",
    "\n",
    "  image_index = 0;\n",
    "  for i in range(nrows):\n",
    "      for j in range(ncols):\n",
    "          axs[i, j].imshow(train_images[image_index])\n",
    "\n",
    "          if len(class_names) > 0:\n",
    "            axs[i, j].set_title(class_names[train_labels[image_index][0]])\n",
    "\n",
    "          image_index += 1\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Affichons 30 images sans les catégories\n",
    "show_some_images(6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Affichons 30 images avec les catégories\n",
    "###On a connu toutes les catégories via ce site : https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "class_names =['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "show_some_images(6, 5, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. La classification multi-classe se caractérise par la capacité d'un modèle à attribuer une entrée à plusieurs catégories ou classes possibles. En d'autres termes, chaque entrée peut prendre la valeur de l'une des nombreuses étiquettes définies.\n",
    "\n",
    "    - L'encodage one-hot consiste à représenter les données sous forme de vecteurs où chaque catégorie correspondante est représentée par 1, tandis que tous les autres restent à 0. (exemple: Le one-hot encoding permet de représenter de manière unique chaque valeur de la variable catégorielle. Par exemple, si vous avez une variable catégorielle qui prend 3 valeurs possibles (”rouge”, ”vert”, ”bleu”), le one-hot encoding vous permettra de représenter chaque valeur sous forme d’un vecteur de 3 éléments, avec un seul élément à 1 et les deux autres à 0. Source: https://medium.com/@sii-lille/data-science-one-hot-encoding-c59e82b3f0e7)\n",
    "\n",
    "    - L'activation softmax est une fonction d'activation qui transforme les valeurs d'entrée en valeurs situées entre 0 et 1. Cette transformation permet d'obtenir des probabilités pour chaque classe, ce qui facilite les calculs et l'interprétation des résultats.\n",
    "\n",
    "    $$\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Création un modèle de Deep learning dense avec 3 couches cachées et une couche de sortie utilisant l'activation Softmax en vue d’apprendre à classifier les images du jeu de données en prenant en compte la couleur des images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Declaration du modele\n",
    "model = models.Sequential()\n",
    "\n",
    "###Aplatir nos données d’entrées\n",
    "train_images = train_images.reshape(-1, 32, 32,  3)\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(32, 32,  3, )))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "###Ajout de la couche de sortie avec le nombre de classes à prédire et la fonction d'activation softmax.\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "###Construire notre modèle\n",
    "model.build(input_shape=(None, 32, 32,  3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) A l'aide de la commande summary(), évaluons et commentons la complexité du\n",
    "modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ce modèle est assez complexe en raison du grand nombre de paramètres à estimer. Cette complexité lui permet d'apprendre des motifs complexes dans les données. Cependant, le nombre élevé de paramètres par rapport au nombre total d'exemples dans l'ensemble de données, qui est de 60 000, pourrait entraîner un risque de sur-apprentissage.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Nous entraînons le modèle en réservant 20% des données comme ensemble de validation. Cet ensemble de validation sera utilisé pour évaluer les performances du modèle pendant l'entraînement et analyser sa performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Compiler le modèle\n",
    "model.compile(\n",
    "    optimizer=optimizers.SGD(learning_rate=0.001),\n",
    "    loss=losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "###Entraîner le modèle\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) L'entraînement du modèle s'est déroulé de manière satisfaisante, avec une amélioration de la précision au fil des époques, indiquant une adaptation continue du modèle et une augmentation de sa performance. De plus, les valeurs de perte ont diminué progressivement au fil des époques, ce qui suggère que le modèle a réussi à réduire les pertes au cours de l'entraînement. \n",
    "Le même schéma se répète également avec les données de validation, où la performance augmente tandis que les pertes diminuent en même temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Pour évaluer le pouvoir prédictif final de notre modèle, nous devons d'abord le tester sur nos données de test. Cette étape nous permettra de comprendre comment le modèle se comportera par rapport à des données qu'il n'a jamais rencontré auparavant. En fonction des performances observées lors de ce test, nous pourrons conclure sur le pouvoir prédictif global de notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Effectuons le test du modèle sur nos donnees de test\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f\"Le pouvoir prédictif de notre modèle est : {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle affiche un pouvoir prédictif de 0.4821000099182129, ce qui équivaut à une précision de prévision d'environ 48,21 %. Cette performance est assez faible car elle se situe en dessous de la barre des 50 %. Cela indique que notre modèle est capable de faire des prédictions qui sont juste légèrement meilleures que le hasard.\n",
    "\n",
    "Un avantage de ce résultat est qu'il suggère que l'entraînement du modèle s'est déroulé de manière efficace. Une précision de prédiction supérieure à 50 % serait idéale, mais ce résultat montre que notre modèle peut faire des prédictions legères sur les données d'entrée non étiquetées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) En examinant nos résultats, nous pouvons conclure que la stratégie consistant à utiliser un réseau de neurones denses pour prédire des images en couleur et de grande résolution peut ne pas être très efficace. Nos résultats montrent que même avec un ensemble de données de 60 000 images de faible résolution, notre performance reste inférieure à 50 %. Cette performance est insuffisante, ce qui suggère que la capacité du modèle à généraliser et à prédire avec précision est limitée et donc en conclusion les réseaux de neurones denses peuvent ne pas être efficaces pour prédire des images d'une grande complexité, comme les images en couleur et de grande résolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Learning - Pratique : Réseaux de neurones convolutifs (Questions de cours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. La philosophie des CNN et son fonctionnement : (Source : ChatGpt)<br>\n",
    "    - Philosophie des Réseaux de Neurones Convolutifs (CNN)<br>\n",
    "        Les CNN s'inspirent du système visuel humain, qui excelle dans la reconnaissance d'images même complexes. Ils sont conçus pour extraire des caractéristiques pertinentes à partir de données d'entrée, principalement des images, afin d'effectuer des tâches de traitement d'images telles que la classification, la segmentation et la détection d'objets.\n",
    "    \n",
    "    - Fonctionnement des CNN\n",
    "        1. Couche d'entrée : L'image d'entrée est représentée sous forme de matrice de pixels.\n",
    "        2. Couches convolutives :\n",
    "            - Des filtres convolutifs glissent sur l'image, produisant des cartes d'activation qui mettent en évidence la présence de caractéristiques spécifiques.\n",
    "            - Une fonction d'activation non linéaire, comme ReLU, est appliquée pour introduire de la non-linéarité dans le réseau.\n",
    "        3. Couches de pooling :\n",
    "            - Réduisent la dimensionnalité des cartes d'activation en effectuant une opération de downsampling, comme le max pooling ou l'average pooling.\n",
    "            - Cela permet de réduire la sensibilité du réseau aux translations et aux déformations locales de l'image.\n",
    "        4. Couches entièrement connectées :\n",
    "            - Aplatissent les cartes d'activation en un vecteur unique.\n",
    "            - Connectent chaque neurone à tous les autres neurones de la couche suivante.\n",
    "            - Produisent une sortie finale, comme la classe d'objet à laquelle appartient l'image.\n",
    "        \n",
    "        5. Rétropropagation :\n",
    "            - Un algorithme est utilisé pour propager l'erreur de la sortie vers les couches précédentes.\n",
    "            - Les paramètres des filtres et des biais sont mis à jour pour minimiser l'erreur et améliorer les performances du réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Les principales différentes entre les CNN et les réseaux denses :<br>\n",
    "    - Les CNN sont très efficaces pour les tâches de vision par ordinateur comme Classification d'images, Détection d'objets, ... alors que pour les réseaux denses ce sont les classifications, la régression, ...\n",
    "\n",
    "    - Les CNN nécessitent plus de données pour l'entraînement que les réseaux denses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Les principales composantes d’une architecture de modèle CNN en donnant les noms des fonctions Python de la librairie Tensorflow/Keras qui implémentent ces composantes :\n",
    "    - **Couches convolutives (tf.keras.layers.Conv2D())**\n",
    "\n",
    "    - **Couches de pooling (tf.keras.layers.MaxPooling2D(), tf.keras.layers.AveragePooling2D())**\n",
    "\n",
    "    - **Couches entièrement connectées (tf.keras.layers.Dense())**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Les CNN et les réseaux denses classiques partagent plusieurs concepts fondamentaux en matière d'apprentissage automatique supervisé, tels que la propagation du gradient, le fonctionnement par couches, l'activation non linéaire, la fonction de perte et l'optimisation. Ces similitudes soulignent les principes fondamentaux de l'apprentissage automatique et mettent en lumière la diversité des architectures de réseaux neuronaux disponibles pour résoudre une variété de problèmes complexes. Bien que les CNN se distinguent par leur capacité à capturer des caractéristiques spatiales dans des données structurées en grille comme les images, les réseaux denses classiques sont efficaces pour des tâches où une connectivité complète entre les neurones est préférable. Ainsi, la disponibilité de ces différentes architectures permet aux praticiens de choisir celle qui convient le mieux à leurs besoins spécifiques en matière d'apprentissage automatique.<br>\n",
    "Source : ChatGpt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
